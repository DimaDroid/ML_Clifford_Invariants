{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed92bc24",
   "metadata": {},
   "source": [
    "This Jupyter Notebook implements a multi-layer perceptron (MLP) classifier using Scikit-learn to classify data linked to A8, D8, and E8 algebras. It imports and organizes invariant coefficient vectors, removes duplicates to create unique datasets, and combines these unique datasets for machine learning. The notebook performs k-fold cross-validation, trains an MLP classifier, and assesses its performance. Three classifiers are trained to differentiate between the invariants specific to one of the three algebras and those associated with the other two, as well as distinguishing them from the fake invariants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef1526c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "from math import floor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc177f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.337714433670044\n"
     ]
    }
   ],
   "source": [
    "# Import data -- in format \n",
    "# [[(permutation order of roots in W definition), [list of invariant coefficient vectors], ...all permutations]\n",
    "\n",
    "# Modified files are used: all 1/2, 3/2, 5/2 are replaced by 0.5, 1.5, 2,5\n",
    "\n",
    "# Extract invarinats' components into a list \n",
    "def parseString(string):\n",
    "    '''Extract permutation as a string and invariants' components as an array'''\n",
    "    tmp = list(string[1:-2].split(\", [[\"))\n",
    "    return [ [float(i) for i in list(elem.split(\", \"))]  for elem in list(tmp[1][:-1].split(\"], [\"))]\n",
    "\n",
    "dataSize = 40320\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create np array for storing permutations and invariants' components\n",
    "data_A8 = np.zeros((dataSize, 9*256), dtype=np.float16)\n",
    "data_D8 = np.zeros((dataSize, 9*256), dtype=np.float16)\n",
    "data_E8 = np.zeros((dataSize, 9*256), dtype=np.float16)\n",
    "\n",
    "# Read data from files, concatenate each invarinat's components\n",
    "lnIdx = 0\n",
    "with open('ADE_Data\\A8inv_Data_mod.txt','r') as file:\n",
    "    for line in file:\n",
    "        data_A8[lnIdx] = np.concatenate(parseString(line.rstrip()))\n",
    "        lnIdx = lnIdx + 1\n",
    "        \n",
    "lnIdx = 0\n",
    "with open('ADE_Data\\D8inv_Data_mod.txt','r') as file:\n",
    "    for line in file:\n",
    "        data_D8[lnIdx] = np.concatenate(parseString(line.rstrip()))\n",
    "        lnIdx = lnIdx + 1\n",
    "\n",
    "lnIdx = 0\n",
    "with open('ADE_Data\\E8inv_Data_mod.txt','r') as file:\n",
    "    for line in file:\n",
    "        data_E8[lnIdx] = np.concatenate(parseString(line.rstrip()))\n",
    "        lnIdx = lnIdx + 1\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3caa67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.029237747192383\n",
      "(40000, 2304)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "fakeDataSize = 40000\n",
    "\n",
    "# Parse string into list of lists of float\n",
    "def parseStringFake(string):\n",
    "    return list(string[1:-1].split(\", \"))\n",
    "\n",
    "# Import fake data\n",
    "fakeData_A8 = np.loadtxt('ADE_Data\\A8inv_Data_Fake.txt',delimiter=',',)\n",
    "fakeData_D8 = np.loadtxt('ADE_Data\\D8inv_Data_Fake.txt',delimiter=',',)\n",
    "fakeData_E8 = np.loadtxt('ADE_Data\\E8inv_Data_Fake.txt',delimiter=',',)\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "print(np.shape(fakeData_A8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1191528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove duplicates from data to create \"unique\" A8/D8/E8 datasets\n",
    "\n",
    "# A8\n",
    "dataSize_unique = 128\n",
    "data_A8_unique = np.zeros((dataSize_unique, 9*256), dtype=np.float16)\n",
    "\n",
    "idx_old = 0\n",
    "idx_new = 0\n",
    "while (idx_old<dataSize) and (idx_new<dataSize_unique):\n",
    "    if not (data_A8[idx_old] == data_A8_unique).all(1).any():\n",
    "        data_A8_unique[idx_new] = data_A8[idx_old]\n",
    "        idx_new = idx_new + 1\n",
    "    idx_old = idx_old + 1\n",
    "    \n",
    "# E8\n",
    "data_E8_unique = np.zeros((dataSize_unique, 9*256), dtype=np.float16)\n",
    "\n",
    "idx_old = 0\n",
    "idx_new = 0\n",
    "while (idx_old<dataSize) and (idx_new<dataSize_unique):\n",
    "    if not (data_E8[idx_old] == data_E8_unique).all(1).any():\n",
    "        data_E8_unique[idx_new] = data_E8[idx_old]\n",
    "        idx_new = idx_new + 1\n",
    "    idx_old = idx_old + 1\n",
    " \n",
    "# D8\n",
    "data_D8_unique = np.zeros((dataSize_unique, 9*256), dtype=np.float16)\n",
    "\n",
    "idx_old = 0\n",
    "idx_new = 0\n",
    "while (idx_old<dataSize) and (idx_new<dataSize_unique):\n",
    "    if not (data_D8[idx_old] == data_D8_unique).all(1).any():\n",
    "        data_D8_unique[idx_new] = data_D8[idx_old]\n",
    "        idx_new = idx_new + 1\n",
    "    idx_old = idx_old + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abd3c28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 2033.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 2730.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 2012.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "\n",
    "# data_A8_unique\n",
    "duplicates = np.zeros(dataSize_unique, dtype=int)\n",
    "\n",
    "for idx1 in tqdm(range(dataSize_unique)):\n",
    "    if duplicates[idx1] == 1:\n",
    "            continue\n",
    "    for idx2 in range(idx1+1,dataSize_unique):\n",
    "        if duplicates[idx2] == 1:\n",
    "            continue\n",
    "        if (data_A8_unique[idx1]==data_A8_unique[idx2]).all():\n",
    "            duplicates[idx2] = 1\n",
    "\n",
    "print(sum(duplicates))\n",
    "\n",
    "# data_D8_unique\n",
    "duplicates = np.zeros(dataSize_unique, dtype=int)\n",
    "\n",
    "for idx1 in tqdm(range(dataSize_unique)):\n",
    "    if duplicates[idx1] == 1:\n",
    "            continue\n",
    "    for idx2 in range(idx1+1,dataSize_unique):\n",
    "        if duplicates[idx2] == 1:\n",
    "            continue\n",
    "        if (data_D8_unique[idx1]==data_D8_unique[idx2]).all():\n",
    "            duplicates[idx2] = 1\n",
    "\n",
    "print(sum(duplicates))\n",
    "\n",
    "# data_E8_unique\n",
    "duplicates = np.zeros(dataSize_unique, dtype=int)\n",
    "\n",
    "for idx1 in tqdm(range(dataSize_unique)):\n",
    "    if duplicates[idx1] == 1:\n",
    "            continue\n",
    "    for idx2 in range(idx1+1,dataSize_unique):\n",
    "        if duplicates[idx2] == 1:\n",
    "            continue\n",
    "        if (data_E8_unique[idx1]==data_E8_unique[idx2]).all():\n",
    "            duplicates[idx2] = 1\n",
    "\n",
    "print(sum(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93ee326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate invariants component only, put them in the list and output data together\n",
    "\n",
    "# Dataset for E8 classification\n",
    "preML_data_A8 = [[data_A8_unique[index], 0] for index in range(dataSize_unique)]\n",
    "preML_data_D8 = [[data_D8_unique[index], 0] for index in range(dataSize_unique)]\n",
    "preML_data_E8 = [[data_E8_unique[index], 1] for index in range(dataSize_unique)]\n",
    "\n",
    "preML_data_FakeA8 = [[fakeData_A8[index], 0] for index in range(fakeDataSize)]\n",
    "preML_data_FakeD8 = [[fakeData_D8[index], 0] for index in range(fakeDataSize)]\n",
    "preML_data_FakeE8 = [[fakeData_E8[index], 0] for index in range(fakeDataSize)]\n",
    "\n",
    "preML_data_Fake = preML_data_FakeA8 + preML_data_FakeD8 + preML_data_FakeE8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7444e357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TrainTestSplit(k, *data):\n",
    "    '''Split each of the data entries into train and test sets in proportion (k-1) to 1. \n",
    "    Concatenate test parts and train parts among themselves. Return shuffled train and test datasets.\n",
    "    k - number of k-fold cross-validations to perform\n",
    "    Input: k - number of k-fold cross-validations to perform, arbitrary number of lists containing pairs (invariant components, label)\n",
    "    Output: input dataset for ML training, corrsponding output dataset, input dataset for ML test, corrsponding output dataset. \n",
    "    Train and test datasets have sizes as (k-1) to 1\n",
    "    '''\n",
    "\n",
    "    #Shuffle data ordering\n",
    "    for dataset in data:\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "    # Unique A8/D8/E8 datasets are much smaller than their \"Fakes\". So we split Unique datasets into test/ train datasets manually to ensure there\n",
    "    # are enough datapoints from Unique datasets in test and train data\n",
    "\n",
    "    # Define data lists, each with k sublists with the relevant data for train/test\n",
    "    ML_data_train = []\n",
    "    ML_data_test = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        preML_data_train = []\n",
    "        preML_data_test = []\n",
    "        \n",
    "        # Split each dataset into train and test parts\n",
    "        for dataset in data:\n",
    "            s = int(floor(len(dataset)/k)) #...number of datapoints in  validation split\n",
    "            preML_data_train.append( [elem for elem in dataset[:i*s]]+[elem for elem in dataset[(i+1)*s:]] )\n",
    "            preML_data_test.append( [elem for elem in dataset[i*s:(i+1)*s]] )\n",
    "\n",
    "        # Combine manually chosen portions of Fake and Original datasets into train and test datasets\n",
    "        ML_data_train.append( [item for sublist in preML_data_train for item in sublist] ) # ... equivalent to ML_data_train.append( preML_data_train[0] + preML_data_train[1] + ... )\n",
    "        # double unique training data experiment\n",
    "        ML_data_test.append( [item for sublist in preML_data_test for item in sublist] ) # ... equivalent to ML_data_test.append( preML_data_test[0] + preML_data_test[1] + ... )\n",
    "\n",
    "        # Shuffle data ordering\n",
    "        np.random.shuffle(ML_data_train[-1])\n",
    "        np.random.shuffle(ML_data_test[-1])\n",
    "        \n",
    "        del(preML_data_train, preML_data_test)\n",
    "    \n",
    "    #Define data lists, each with k sublists with the relevant data for training and cross-validation\n",
    "    Train_inputs, Train_outputs, Test_inputs, Test_outputs = [], [], [], []\n",
    "\n",
    "    for i in range(k):\n",
    "        Train_inputs.append([datapoint[0] for datapoint in ML_data_train[i]])\n",
    "        Train_outputs.append([datapoint[1] for datapoint in ML_data_train[i]])\n",
    "        Test_inputs.append([datapoint[0] for datapoint in ML_data_test[i]])\n",
    "        Test_outputs.append([datapoint[1] for datapoint in ML_data_test[i]])\n",
    "\n",
    "    del(ML_data_train, ML_data_test) # data no longer needed\n",
    "    \n",
    "    return Train_inputs, Train_outputs, Test_inputs, Test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3abd29cd-585d-42ea-9935-124ffb6c48e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 4   #...number of k-fold cross-validations to perform (k = 5 => 80(train) : 20(test) splits approx.)\n",
    "Train_inputs, Train_outputs, Test_inputs, Test_outputs = TrainTestSplit(k, preML_data_A8, preML_data_D8, preML_data_E8, preML_data_Fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e714e08b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Measures for E8 classification:\n",
      "Accuracy:  0.9996428096757044 \\pm 3.596929009604529e-05\n",
      "Recall:  0.671875 \\pm 0.0390625\n",
      "Precision:  0.9903846153846154 \\pm 0.008327167344081135\n",
      "F1:  0.7973341526301838 \\pm 0.025618069821102398\n"
     ]
    }
   ],
   "source": [
    "# Run NN train & test\n",
    "# Define measure lists\n",
    "F1s, ACCs, PRECs, RECs = [], [], [], []    #...lists of measures\n",
    "seed = 1                                   #...select a random seeding (any integer) for regressor initialisation\n",
    "\n",
    "#Loop through each cross-validation run\n",
    "for i in range(k):\n",
    "    #Define & Train NN Regressor directly on the data\n",
    "    nn_clf = MLPClassifier((256,), activation='relu', solver='adam', alpha=0.001, n_iter_no_change=5, random_state=seed)  #...can edit the NN structure here\n",
    "    nn_clf.fit(Train_inputs[i], Train_outputs[i]) \n",
    "    \n",
    "    #Compute NN predictions on test data, and calculate learning measures\n",
    "    Test_pred = nn_clf.predict(Test_inputs[i])\n",
    "    F1s.append(f1_score(Test_outputs[i], Test_pred))\n",
    "    ACCs.append(accuracy_score(Test_outputs[i], Test_pred))\n",
    "    PRECs.append(precision_score(Test_outputs[i], Test_pred))\n",
    "    RECs.append(recall_score(Test_outputs[i], Test_pred))\n",
    "    \n",
    "    #plot(nn_clf.loss_curve_)\n",
    "    plt.show()\n",
    "                \n",
    "# Averaged output learning measures\n",
    "print('####################################')\n",
    "print('Measures for E8 classification:')\n",
    "print('Accuracy: ',sum(ACCs)/k,'\\pm',np.std(ACCs)/np.sqrt(k))\n",
    "print('Recall: ',sum(RECs)/k,'\\pm',np.std(RECs)/np.sqrt(k))\n",
    "print('Precision: ',sum(PRECs)/k,'\\pm',np.std(PRECs)/np.sqrt(k))\n",
    "print('F1: ',sum(F1s)/k,'\\pm',np.std(F1s)/np.sqrt(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e795264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLear space for other classifiers\n",
    "del(nn_clf, Train_inputs, Train_outputs, Test_inputs, Test_outputs, Test_pred, F1s, ACCs, RECs, PRECs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4915db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732fdabb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate invariants component only, put them in the list and output data together\n",
    "\n",
    "# Dataset for D8 classification\n",
    "preML_data_A8 = [[data_A8_unique[index], 0] for index in range(dataSize_unique)]\n",
    "preML_data_D8 = [[data_D8_unique[index], 1] for index in range(dataSize_unique)]\n",
    "preML_data_E8 = [[data_E8_unique[index], 0] for index in range(dataSize_unique)]\n",
    "\n",
    "preML_data_FakeA8 = [[fakeData_A8[index], 0] for index in range(fakeDataSize)]\n",
    "preML_data_FakeD8 = [[fakeData_D8[index], 0] for index in range(fakeDataSize)]\n",
    "preML_data_FakeE8 = [[fakeData_E8[index], 0] for index in range(fakeDataSize)]\n",
    "\n",
    "preML_data_Fake = preML_data_FakeA8 + preML_data_FakeD8 + preML_data_FakeE8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd50dac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 5   #...number of k-fold cross-validations to perform (k = 5 => 80(train) : 20(test) splits approx.)\n",
    "Train_inputs, Train_outputs, Test_inputs, Test_outputs = TrainTestSplit(k, preML_data_A8, preML_data_D8, preML_data_E8, preML_data_Fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97598941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Measures for D8 classification:\n",
      "Accuracy:  0.9996593977154726 \\pm 5.0395018527446654e-05\n",
      "Recall:  0.712 \\pm 0.028621670111997306\n",
      "Precision:  0.95 \\pm 0.03464101615137754\n",
      "F1:  0.8124283103296148 \\pm 0.027401256341682823\n"
     ]
    }
   ],
   "source": [
    "# Run NN train & test\n",
    "# Define measure lists\n",
    "F1s, ACCs, PRECs, RECs = [], [], [], []    #...lists of measures\n",
    "seed = 1                          #...select a random seeding (any integer) for regressor initialisation\n",
    "\n",
    "#Loop through each cross-validation run\n",
    "for i in range(k):\n",
    "    #Define & Train NN Regressor directly on the data\n",
    "    nn_clf = MLPClassifier((256,64), activation='relu', solver='adam', alpha=0.001, n_iter_no_change=5, random_state=seed)  #...can edit the NN structure here\n",
    "    nn_clf.fit(Train_inputs[i], Train_outputs[i]) \n",
    "\n",
    "    #Compute NN predictions on test data, and calculate learning measures\n",
    "    Test_pred = nn_clf.predict(Test_inputs[i])\n",
    "    F1s.append(f1_score(Test_outputs[i], Test_pred))\n",
    "    ACCs.append(accuracy_score(Test_outputs[i], Test_pred))\n",
    "    PRECs.append(precision_score(Test_outputs[i], Test_pred))\n",
    "    RECs.append(recall_score(Test_outputs[i], Test_pred))\n",
    "                \n",
    "# Averaged output learning measures\n",
    "print('####################################')\n",
    "print('Measures for D8 classification:')\n",
    "print('Accuracy: ',sum(ACCs)/k,'\\pm',np.std(ACCs)/np.sqrt(k))\n",
    "print('Recall: ',sum(RECs)/k,'\\pm',np.std(RECs)/np.sqrt(k))\n",
    "print('Precision: ',sum(PRECs)/k,'\\pm',np.std(PRECs)/np.sqrt(k))\n",
    "print('F1: ',sum(F1s)/k,'\\pm',np.std(F1s)/np.sqrt(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b35eb420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLear space for other classifiers\n",
    "del(nn_clf, Train_inputs, Train_outputs, Test_inputs, Test_outputs, Test_pred, F1s, ACCs, RECs, PRECs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706dcaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f35bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate invariants component only, put them in the list and output data together\n",
    "\n",
    "# Dataset for A8 classification\n",
    "preML_data_A8 = [[data_A8_unique[index], 1] for index in range(dataSize_unique)]\n",
    "preML_data_D8 = [[data_D8_unique[index], 0] for index in range(dataSize_unique)]\n",
    "preML_data_E8 = [[data_E8_unique[index], 0] for index in range(dataSize_unique)]\n",
    "\n",
    "preML_data_FakeA8 = [[fakeData_A8[index], 0] for index in range(fakeDataSize)]\n",
    "preML_data_FakeD8 = [[fakeData_D8[index], 0] for index in range(fakeDataSize)]\n",
    "preML_data_FakeE8 = [[fakeData_E8[index], 0] for index in range(fakeDataSize)]\n",
    "\n",
    "preML_data_Fake = preML_data_FakeA8 + preML_data_FakeD8 + preML_data_FakeE8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f1366bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 5   #...number of k-fold cross-validations to perform (k = 5 => 80(train) : 20(test) splits approx.)\n",
    "Train_inputs, Train_outputs, Test_inputs, Test_outputs = TrainTestSplit(k, preML_data_A8, preML_data_D8, preML_data_E8, preML_data_Fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5132fcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Measures for A8 classification:\n",
      "Accuracy:  0.9994434060228452 \\pm 4.170268045030241e-05\n",
      "Recall:  0.52 \\pm 0.046647615158762396\n",
      "Precision:  0.910448717948718 \\pm 0.026773638992573796\n",
      "F1:  0.6542861989075083 \\pm 0.033215009131912686\n"
     ]
    }
   ],
   "source": [
    "# Run NN train & test\n",
    "# Define measure lists\n",
    "F1s, ACCs, PRECs, RECs = [], [], [], []    #...lists of measures\n",
    "seed = 1                          #...select a random seeding (any integer) for regressor initialisation\n",
    "\n",
    "#Loop through each cross-validation run\n",
    "for i in range(k):\n",
    "    #Define & Train NN Regressor directly on the data\n",
    "    nn_clf = MLPClassifier((256,64), activation='relu', solver='adam', alpha=0.001, n_iter_no_change=5, random_state=seed)  #...can edit the NN structure here\n",
    "    nn_clf.fit(Train_inputs[i], Train_outputs[i]) \n",
    "\n",
    "    #Compute NN predictions on test data, and calculate learning measures\n",
    "    Test_pred = nn_clf.predict(Test_inputs[i])\n",
    "    F1s.append(f1_score(Test_outputs[i], Test_pred))\n",
    "    ACCs.append(accuracy_score(Test_outputs[i], Test_pred))\n",
    "    PRECs.append(precision_score(Test_outputs[i], Test_pred))\n",
    "    RECs.append(recall_score(Test_outputs[i], Test_pred))\n",
    "                \n",
    "# Averaged output learning measures\n",
    "print('####################################')\n",
    "print('Measures for A8 classification:')\n",
    "print('Accuracy: ',sum(ACCs)/k,'\\pm',np.std(ACCs)/np.sqrt(k))\n",
    "print('Recall: ',sum(RECs)/k,'\\pm',np.std(RECs)/np.sqrt(k))\n",
    "print('Precision: ',sum(PRECs)/k,'\\pm',np.std(PRECs)/np.sqrt(k))\n",
    "print('F1: ',sum(F1s)/k,'\\pm',np.std(F1s)/np.sqrt(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aed8d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLear space for other classifiers\n",
    "del(nn_clf, Train_inputs, Train_outputs, Test_inputs, Test_outputs, Test_pred, F1s, ACCs, RECs, PRECs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b8ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
